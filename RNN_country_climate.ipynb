{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "from GHCND import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter the stations used, only stations with fewer than 100 missing data points were considered. The number of missing values is calculated in the script `get_station_counts.py` (took ~50 minutes to run on my laptop). These counts are stored in the file `stat_counts_tmax.json` in the json format for ease of future loading, as this script need only be run once for a given set of stations and data. The file is opened and read below, then all stations with 100 or fewer missing data points saved in a seperate list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations found: 103\n",
      "ASN00003003\n"
     ]
    }
   ],
   "source": [
    "# open json file containing the number of data gaps for the stations\n",
    "f = open('data/stat_counts_tmax.json')\n",
    "data = json.load(f)\n",
    "\n",
    "# find all stations with no data gaps\n",
    "no_gaps_tmax = [name for name, count in data.items() if count <= 100]\n",
    "print(f\"Number of stations found: {len(no_gaps_tmax)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve data for the given station and variable. An instance of the GHCND class is created, then the `readCountriesFile` and `readStationsFile` methods called on it. These methods extract information on the countries and stations available and stores these in fields as dictionaries. The list of station names is stored as a seperate list for future use.\n",
    "\n",
    "The station to be investigated is set from the list of those with few missing date points. The required file name is constructed from the station name, which is then used to build the url required to fetch the data from the course-provided web directory of data files. The required data file is then copied from this remote directory to a local one (`/data`). The `processFile` method is then called on the GHCND class instance to extract the data from this file into a dictionary.\n",
    "\n",
    "Data for the variable specified is extracted from this dictionary into an instance of the `Variable` class, with fields for the values and their corresponding dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 219 countries and codes\n",
      "Read 991 stations with justGSN\n",
      "Station selected: ASN00003003\n",
      "Filename: ASN00003003.dly\n",
      "url name: http://www.hep.ucl.ac.uk/undergrad/0056/other/projects/ghcnd/ghcnd_gsn/ASN00003003.dly\n",
      "destination: data/ASN00003003.dly\n",
      "Station details: ASN00003003 is BROOME AIRPORT, Australia at -17.9475 122.2353 7.4\n"
     ]
    }
   ],
   "source": [
    "# create instance of the GHCND class and extract information on countries and stations from their respective files\n",
    "ghn = GHCND()\n",
    "ghn.readCountriesFile()\n",
    "ghn.readStationsFile()\n",
    "\n",
    "# get list of station names\n",
    "station_names = ghn.getStatKeyNames()\n",
    "\n",
    "# set station to be investigated\n",
    "station = no_gaps_tmax[0]\n",
    "print(f\"Station selected: {station}\")\n",
    "\n",
    "# get url for a specified station\n",
    "fileName = f\"{station}.dly\"\n",
    "print(f\"Filename: {fileName}\")\n",
    "urlName = f\"http://www.hep.ucl.ac.uk/undergrad/0056/other/projects/ghcnd/ghcnd_gsn/{fileName}\"\n",
    "print(f\"url name: {urlName}\")\n",
    "\n",
    "# copy station data from remote to local\n",
    "destination = f\"data/stations_daily/{fileName}\"\n",
    "print(f\"destination: {destination}\")\n",
    "urllib.request.urlretrieve(urlName, destination)\n",
    "station_data = ghn.processFile(destination)\n",
    "print(f\"Station details: {ghn.getStation(station)}\")\n",
    "\n",
    "# extract data for specified variable into an instance of the Variable class\n",
    "t_max = Variable(ghn.getVar(station_data, 'TMAX'), \"max temp (degC)\", ghn.stationDict[station].name)\n",
    "dates = t_max.get_dates()\n",
    "vals = t_max.get_vals()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this section is concerend with the climate at a given station rather than the weather, the monthly mean for the specified variable is found by calling the `get_monthly_means` method on the instance of the Variable class. To improve the performance and training stavility of the model, he date are then normalised by calling the `normalise` method, which subtracts the mean of the entire dataset then subtracts the maximum value for each data point. The first ten months of these mean values and normalised mean values are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean values for each month of data\n",
    "means = np.array(t_max.get_monthly_means())\n",
    "print(f\"Number of months of data: {len(means)}\")\n",
    "\n",
    "# normalise means\n",
    "means_normalised = t_max.normalise(means)\n",
    "\n",
    "# plot means and normalised means\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(means[:12*10])\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Mean monthly maximum temperature (degC)\")\n",
    "ax.set_title(\"Mean and normalised mean monthly maximum temperatures over 10 years\")\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(means_normalised[:(12*10)])\n",
    "ax2.set_ylabel(\"Normalised mean monthly maximum temperature\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are divided into training, validating, and testing datasets. The training dataset must be the largest in order to provide sufficient material for the model to achieve a good performance. The testing dataset is smaller, but large enough that the model has sufficient input for predicting future sequences. The validation dataset is the smallest. A training:testing:validation ratio of 0.7:0.2:0.1 is used.\n",
    "\n",
    "The offset describes how far into the future the model will predict. An offset of 12 is used here, meaning that the model will predict twelve months ahead. The window size dictates how much data the model is trained on in a single batch. If this is too small, the model will not perform well, however it must be smaller than the validation dataset minus the offset, otherwise it will be impossible to shape the dataset as necessary in later steps. It was found during testing that a window size of 80 worked well and gave a good performance.\n",
    "\n",
    "The divided datasets are then split into a series of overlapping windows with length `WINDOW_SIZE`, and an associated value `OFFSET` points further ahead in the time series. These are the input windows and targets on which the model will be trained, validated and tested.\n",
    "\n",
    "These windows are reshaped from (number of windows, window size) to (number of windows, window size, number of features), where 'number of features' is the number of variables being used in this model's training, in this case, one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of months of training data: 1089\n",
      "Number of months of validation data: 155\n",
      "Number of months of testing data: 311\n",
      "input shape: (220, 80); label shape: (220, 12)\n",
      "input shape: (997, 80); label shape: (997, 12)\n",
      "input shape: (64, 80); label shape: (64, 12)\n"
     ]
    }
   ],
   "source": [
    "OFFSET = 12\n",
    "\n",
    "# calculate appropriate divisions of data\n",
    "test_len = int(len(means_normalised) * 0.2)\n",
    "train_len = int(len(means_normalised) * 0.7)\n",
    "validate_len = int(len(means_normalised) * 0.1)\n",
    "\n",
    "# if the validation dataset is too small, use a smaller window size\n",
    "if validate_len < 80 + OFFSET + 1:\n",
    "    WINDOW_SIZE = validate_len - OFFSET - 1\n",
    "else:\n",
    "    WINDOW_SIZE = 80\n",
    "\n",
    "print(f\"Number of months of training data: {train_len}\")\n",
    "print(f\"Number of months of validation data: {validate_len}\")\n",
    "print(f\"Number of months of testing data: {test_len}\")\n",
    "\n",
    "# divide data into training, validating and testing sets\n",
    "means_test = means_normalised[:test_len]\n",
    "means_train = means_normalised[test_len+1:test_len+train_len]\n",
    "means_validate = means_normalised[test_len+train_len+1:]\n",
    "\n",
    "# split data into input windows and targets\n",
    "input_test, target_test = shapeArray(means_test, WINDOW_SIZE, OFFSET)\n",
    "input_train, target_train = shapeArray(means_train, WINDOW_SIZE, OFFSET)\n",
    "input_validate, target_validate = shapeArray(means_validate, WINDOW_SIZE, OFFSET)\n",
    "\n",
    "# reshape the data into the correct format for input into the model\n",
    "n_features = 1\n",
    "input_train = input_train.reshape((input_train.shape[0], input_train.shape[1], n_features))\n",
    "input_test = input_test.reshape((input_test.shape[0], input_test.shape[1], n_features))\n",
    "input_validate = input_validate.reshape((input_validate.shape[0], input_validate.shape[1], n_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger window size makes the loss larger at the beginning, but makes the validation loss closer to the training loss - better at predicting unseen data.\n",
    "Good window size: 80 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(64, input_shape = (WINDOW_SIZE, 1), activation = 'relu', return_sequences = False))\n",
    "model.add(layers.Dense(1, activation = \"linear\"))\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# train model and extract final loss\n",
    "history = model.fit(input_train, target_train, epochs = 100, validation_data = (input_validate, target_validate))\n",
    "cost = history.history['loss']\n",
    "val_cost = history.history['val_loss']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(cost)\n",
    "ax.plot(val_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(input_test)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(prediction)\n",
    "ax.plot(target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot first 10 years of data against predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
