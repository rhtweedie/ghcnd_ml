{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "from GHCND import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, validate and test\n",
    "# get data into correct shape (double check!!!)\n",
    "# normalisation - scale to be between -1 and 1 (divide by max?)\n",
    "# train and validate model\n",
    "# test on input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/stat_counts_tmax.txt')\n",
    "data = json.load(f)\n",
    "\n",
    "# find all stations with no data gaps\n",
    "no_gaps_tmax = [k for k, v in data.items() if v == 0]\n",
    "station = no_gaps_tmax[-1]\n",
    "print(station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data, and shape train, validate and test arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghn = GHCND()\n",
    "ghn.readCountriesFile()\n",
    "ghn.readStationsFile()\n",
    "\n",
    "# Get list of station names\n",
    "station_names = ghn.getStatKeyNames()\n",
    "\n",
    "# get url for a given station index\n",
    "fileName = f\"{station}.dly\"\n",
    "print(f\"Filename: {fileName}\")\n",
    "urlName = f\"http://www.hep.ucl.ac.uk/undergrad/0056/other/projects/ghcnd/ghcnd_gsn/{fileName}\"\n",
    "print(f\"url name: {urlName}\")\n",
    "\n",
    "# copy station data from remote to local\n",
    "destination = f\"data/{fileName}\"\n",
    "print(f\"destination: {destination}\")\n",
    "urllib.request.urlretrieve(urlName, destination)\n",
    "station_data = ghn.processFile(destination)\n",
    "print(ghn.getStation(station))\n",
    "\n",
    "t_max = Variable(ghn.getVar(station_data, 'TMAX'), \"max temp (degC)\", ghn.stationDict[station].name)\n",
    "t_max.convert_time()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t_max.get_dates(), t_max.get_vals())\n",
    "ax.set_xlabel(\"Days since first recording\")\n",
    "ax.set_ylabel(f\"{t_max.get_label()}\")\n",
    "ax.legend((t_max.get_station()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = t_max.get_vals()\n",
    "vals = t_max.normalise(vals)\n",
    "WINDOW_SIZE = 10\n",
    "OFFSET = 365\n",
    "\n",
    "# reshape data into input windows and targets\n",
    "input, target = shapeArray(vals, WINDOW_SIZE, OFFSET)\n",
    "print(f\"Vals shape: {np.shape(vals)}\")\n",
    "print(f\"Input shape: {np.shape(input)}\")\n",
    "print(f\"Target shape: {np.shape(target)}\")\n",
    "\n",
    "# divide reshaoed data into training, vaildation and testing data\n",
    "train_len = int(len(input) * 0.7)\n",
    "validate_len = int(len(input) * 0.2)\n",
    "test_len = int(len(input) * 0.1)\n",
    "\n",
    "input_train = input[:train_len]\n",
    "input_validate = input[train_len+1:train_len+validate_len]\n",
    "input_test = input[train_len+validate_len+1:]\n",
    "\n",
    "target_train = target[:train_len]\n",
    "target_validate = target[train_len+1:train_len+validate_len]\n",
    "target_test = target[train_len+validate_len+1:]\n",
    "\n",
    "print(f\"Training input shape: {np.shape(input_train)}\")\n",
    "print(input_train[0:4])\n",
    "print(f\"Training target shape: {np.shape(target_train)}\")\n",
    "print(target_train[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(64, input_shape = (None, 1), return_sequences = True)) # LSTM layer with 50 neurons\n",
    "model.add(layers.LSTM(16, activation = 'linear', return_sequences = True))\n",
    "model.add(layers.LSTM(4, activation = 'linear', return_sequences = False))\n",
    "model.add(layers.Dense(128, activation = \"linear\"))\n",
    "model.add(layers.Dense(1, activation = \"linear\"))\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# train model and extract final loss\n",
    "history = model.fit(input_train, target_train, epochs = 30)\n",
    "cost = history.history['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least mean-squared\n",
    "\n",
    "Sum of the squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_lms = weather_fake_loss(vals)\n",
    "print(fake_lms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fake lms, i.e. assuming that the weather the next day will be the same as the one before, is 30,000 times greater than the loss achieved by the RNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
